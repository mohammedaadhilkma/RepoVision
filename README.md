# RepoVision â€“ GitHub Repository Explainer AI

<div align="center">

![RepoVision Banner](https://img.shields.io/badge/RepoVision-AI%20Powered-blue?style=for-the-badge&logo=github)
![License](https://img.shields.io/badge/license-MIT-green?style=for-the-badge)
![Python](https://img.shields.io/badge/Python-3.10+-blue?style=for-the-badge&logo=python)
![React](https://img.shields.io/badge/React-18-61DAFB?style=for-the-badge&logo=react)
![Ollama](https://img.shields.io/badge/Ollama-Local%20LLM-orange?style=for-the-badge)

**Analyze any public GitHub repository with local AI. Get instant documentation, architecture diagrams, tech stack analysis, and improvement suggestions â€” 100% free.**

</div>

---

## ğŸ¯ Features

- ğŸ” **Deep Repo Analysis** â€“ Languages, frameworks, databases, dependencies
- ğŸ¤– **AI-Powered Insights** â€“ Powered by Ollama (Mistral/Llama3) running locally
- ğŸ“Š **Architecture Diagrams** â€“ 3 Mermaid.js diagrams (Architecture, Component, Flow)
- ğŸ“ **Folder Tree View** â€“ Visual file structure explorer
- ğŸ“ˆ **Complexity Scoring** â€“ Automated complexity & code quality scores
- ğŸ›¡ï¸ **Security Analysis** â€“ Basic security risk detection
- ğŸ’¡ **AI Suggestions** â€“ Improvement recommendations
- ğŸ“„ **PDF Export** â€“ Download full analysis as PDF
- âš¡ **Fallback Mode** â€“ Works even without Ollama (rule-based analysis)

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RepoVision                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚   React Frontend (Vite + TailwindCSS)               â”‚
â”‚         â”‚                                           â”‚
â”‚         â–¼                                           â”‚
â”‚   FastAPI Backend (Python)                          â”‚
â”‚         â”‚                                           â”‚
â”‚         â”œâ”€â”€â–º GitHub Repo Clone (GitPython)          â”‚
â”‚         â”‚         â”‚                                 â”‚
â”‚         â”‚         â–¼                                 â”‚
â”‚         â”‚    Repo Analyzer                          â”‚
â”‚         â”‚    (Languages, Frameworks, Tree)          â”‚
â”‚         â”‚                                           â”‚
â”‚         â””â”€â”€â–º Ollama Local LLM (Mistral/Llama3)      â”‚
â”‚                   â”‚                                 â”‚
â”‚                   â–¼                                 â”‚
â”‚         JSON Response + Mermaid Diagrams            â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| Frontend | React 18, Vite, TailwindCSS, Mermaid.js, Axios |
| Backend | Python, FastAPI, Uvicorn |
| AI | Ollama (Mistral / Llama3) |
| Repo Analysis | GitPython |
| PDF Export | jsPDF + html2canvas |
| Deployment | Vercel (frontend) + Render (backend) |

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Node.js 18+
- Git
- [Ollama](https://ollama.ai) (for AI features)

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/repovision.git
cd repovision
```

### 2. Set Up Ollama (Free Local AI)

```bash
# Install Ollama from https://ollama.ai
# Then pull a model:
ollama pull mistral
# OR
ollama pull llama3

# Start Ollama server
ollama serve
```

### 3. Start the Backend

```bash
cd backend

# Create virtual environment
python -m venv venv
venv\Scripts\activate        # Windows
# source venv/bin/activate   # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Copy environment config
copy .env.example .env       # Windows
# cp .env.example .env       # Linux/Mac

# Start the server
uvicorn main:app --reload --port 8000
```

Backend will be available at: `http://localhost:8000`
API docs at: `http://localhost:8000/docs`

### 4. Start the Frontend

```bash
cd frontend

# Install dependencies
npm install

# Start dev server
npm run dev
```

Frontend will be available at: `http://localhost:5173`

---

## âš™ï¸ Environment Variables

### Backend (`backend/.env`)

| Variable | Default | Description |
|----------|---------|-------------|
| `OLLAMA_BASE_URL` | `http://localhost:11434` | Ollama server URL |
| `OLLAMA_MODEL` | `mistral` | LLM model to use |
| `TEMP_CLONE_DIR` | `./temp_repos` | Temp directory for cloning |
| `MAX_REPO_SIZE_MB` | `200` | Max repo size to analyze |

### Frontend (`frontend/.env`)

| Variable | Default | Description |
|----------|---------|-------------|
| `VITE_API_URL` | `http://localhost:8000` | Backend API URL |

---

## ğŸŒ Deployment (Free Tier)

### Frontend â†’ Vercel

```bash
cd frontend
npm run build
# Deploy dist/ folder to Vercel
# Set VITE_API_URL to your backend URL
```

### Backend â†’ Render

1. Create a new Web Service on [render.com](https://render.com)
2. Connect your GitHub repo
3. Set build command: `pip install -r requirements.txt`
4. Set start command: `uvicorn main:app --host 0.0.0.0 --port $PORT`
5. Add environment variables from `.env.example`

> **Note**: Ollama cannot run on Render free tier. For cloud deployment, use a VPS with Ollama installed, or the app will automatically use rule-based fallback analysis.

---

## ğŸ“ Project Structure

```
repovision/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                    # FastAPI app entry point
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ schemas.py             # Pydantic request/response models
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ repo_analyzer.py       # GitHub repo cloning & analysis
â”‚   â”‚   â””â”€â”€ llm_service.py         # Ollama LLM integration
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ file_utils.py          # Language detection, tree builder
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx                # Main app component
â”‚   â”‚   â”œâ”€â”€ main.jsx               # React entry point
â”‚   â”‚   â”œâ”€â”€ index.css              # Global styles + TailwindCSS
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ InputSection.jsx   # URL input + analyze button
â”‚   â”‚       â”œâ”€â”€ LoadingAnimation.jsx # Step-by-step loading UI
â”‚   â”‚       â”œâ”€â”€ SummaryCard.jsx    # Project summary display
â”‚   â”‚       â”œâ”€â”€ TechStackCards.jsx # Languages/frameworks/databases
â”‚   â”‚       â”œâ”€â”€ MermaidDiagram.jsx # Architecture diagram renderer
â”‚   â”‚       â”œâ”€â”€ FolderTree.jsx     # File structure viewer
â”‚   â”‚       â”œâ”€â”€ ScoreSection.jsx   # Complexity & quality scores
â”‚   â”‚       â”œâ”€â”€ ImprovementsSection.jsx # AI suggestions & security
â”‚   â”‚       â””â”€â”€ DownloadPDF.jsx    # PDF export
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â””â”€â”€ postcss.config.js
â”‚
â””â”€â”€ README.md
```

---

## ğŸ”® Future Improvements

- [ ] Support private repositories (GitHub token auth)
- [ ] Real-time streaming analysis with WebSockets
- [ ] Compare two repositories side-by-side
- [ ] Export as Markdown documentation
- [ ] GitHub Actions integration
- [ ] Support for GitLab and Bitbucket
- [ ] Caching analyzed repos to avoid re-cloning
- [ ] User accounts and analysis history

---

## ğŸ“„ License

MIT License â€“ free to use, modify, and distribute.

---

<div align="center">
Built with â¤ï¸ using FastAPI, React, and Ollama
</div>
#   R e p o V i s i o n  
 #   R e p o V i s i o n  
 